name: GPU CI Testing

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  vllm-deployment-test:
    runs-on: ubuntu-latest  

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          ref: ${{ github.head_ref }}

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'

      - name: Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e .
          pip install vllm
        # Installs all Python dependencies including those needed by vLLM and your project[3]

      - name: Start vLLM Server
        run: |
          nohup dynasor-vllm --model deepseek-ai/DeepSeek-R1-Distill-Qwen-7B --enable-prefix-caching > server.log 2>&1 &
        # The server command is run in background so that the CI job can proceed to test it[1]

      - name: Wait for Server Startup
        run: sleep 240

      - name: Test the API Endpoint
        run: |
          python examples/client-vllm.py

      # - name: Stop vLLM Server
      #   run: |
      #     # Cleanly shuts down the server process running in the background
      #     pkill -f dynasor-vllm || true
